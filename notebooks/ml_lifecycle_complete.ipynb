{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# LifeArc ML Pipeline - Complete Lifecycle in Snowflake\n",
    "\n",
    "## End-to-End Machine Learning for Drug Discovery\n",
    "\n",
    "This notebook demonstrates **production-grade ML workflows** using native Snowflake ML capabilities:\n",
    "\n",
    "| Stage | Snowflake Capability | Why It Matters |\n",
    "|-------|---------------------|----------------|\n",
    "| 1. Discovery | Snowpark DataFrames | Scalable EDA on large datasets |\n",
    "| 2. Feature Engineering | **Snowflake Feature Store** | Centralized, versioned, point-in-time correct |\n",
    "| 3. Training | **Experiments** | Track runs, compare hyperparameters |\n",
    "| 4. Registry | **Snowflake Model Registry** | Version control, aliases, lifecycle |\n",
    "| 5. Deployment | Model Serving | Inference at scale |\n",
    "| 6. Monitoring | **ML Observability** | Drift detection, alerting |\n",
    "| 7. Lineage | **ML Lineage** | End-to-end traceability for compliance |\n",
    "\n",
    "### Use Case: Clinical Trial Outcome Prediction\n",
    "\n",
    "Predict patient response category (Complete Response, Partial Response, Stable Disease, Progressive Disease) based on:\n",
    "- Biomarker status\n",
    "- Treatment arm\n",
    "- Trial phase\n",
    "- Target gene\n",
    "- ctDNA confirmation\n",
    "\n",
    "**Business Value**: Patient stratification for trial enrollment optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Environment Setup & Connection\n",
    "\n",
    "Connect to Snowflake and import native ML libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Snowpark\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.types import *\n",
    "\n",
    "# Snowflake ML - Native APIs\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore, \n",
    "    Entity, \n",
    "    FeatureView,\n",
    "    CreationMode\n",
    ")\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.modeling.preprocessing import OneHotEncoder, StandardScaler\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.xgboost import XGBClassifier\n",
    "from snowflake.ml.modeling.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"✓ Snowflake ML libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection - In Snowflake Notebooks, use get_active_session()\n",
    "# For local development, use connection parameters\n",
    "\n",
    "try:\n",
    "    # Running in Snowflake Notebook\n",
    "    from snowflake.snowpark.context import get_active_session\n",
    "    session = get_active_session()\n",
    "    print(\"✓ Using active Snowflake session\")\n",
    "except:\n",
    "    # Running locally - use key-pair auth\n",
    "    import os\n",
    "    from cryptography.hazmat.primitives import serialization\n",
    "    from cryptography.hazmat.backends import default_backend\n",
    "    \n",
    "    # Load private key\n",
    "    key_path = os.environ.get('SNOWFLAKE_PRIVATE_KEY_PATH', '~/.snowflake/rsa_key.p8')\n",
    "    with open(os.path.expanduser(key_path), 'rb') as f:\n",
    "        p_key = serialization.load_pem_private_key(\n",
    "            f.read(),\n",
    "            password=os.environ.get('SNOWFLAKE_PRIVATE_KEY_PASSPHRASE', '').encode() or None,\n",
    "            backend=default_backend()\n",
    "        )\n",
    "    \n",
    "    pkb = p_key.private_bytes(\n",
    "        encoding=serialization.Encoding.DER,\n",
    "        format=serialization.PrivateFormat.PKCS8,\n",
    "        encryption_algorithm=serialization.NoEncryption()\n",
    "    )\n",
    "    \n",
    "    connection_params = {\n",
    "        \"account\": os.environ.get('SNOWFLAKE_ACCOUNT'),\n",
    "        \"user\": os.environ.get('SNOWFLAKE_USER'),\n",
    "        \"private_key\": pkb,\n",
    "        \"warehouse\": \"COMPUTE_WH\",\n",
    "        \"database\": \"LIFEARC_POC\",\n",
    "        \"schema\": \"ML_DEMO\"\n",
    "    }\n",
    "    session = Session.builder.configs(connection_params).create()\n",
    "    print(\"✓ Connected via key-pair authentication\")\n",
    "\n",
    "# Verify connection\n",
    "print(f\"Database: {session.get_current_database()}\")\n",
    "print(f\"Schema: {session.get_current_schema()}\")\n",
    "print(f\"Warehouse: {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discovery-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Data Discovery & EDA\n",
    "\n",
    "Explore the clinical trial data to understand feature candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clinical trial results\n",
    "clinical_df = session.table(\"LIFEARC_POC.DATA_SHARING.CLINICAL_TRIAL_RESULTS\")\n",
    "\n",
    "print(f\"Total records: {clinical_df.count():,}\")\n",
    "print(f\"\\nSchema:\")\n",
    "for field in clinical_df.schema.fields:\n",
    "    print(f\"  {field.name}: {field.datatype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response category distribution (target variable)\n",
    "response_dist = clinical_df.group_by(\"RESPONSE_CATEGORY\").count().order_by(\"COUNT\", ascending=False)\n",
    "response_dist.show()\n",
    "\n",
    "# This is what we're predicting - patient response to treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore feature distributions\n",
    "print(\"=== Biomarker Status ===\")\n",
    "clinical_df.group_by(\"BIOMARKER_STATUS\").agg(\n",
    "    F.count(\"*\").alias(\"COUNT\"),\n",
    "    F.avg(\"PFS_MONTHS\").alias(\"AVG_PFS\")\n",
    ").show()\n",
    "\n",
    "print(\"\\n=== ctDNA Confirmation ===\")\n",
    "clinical_df.group_by(\"CTDNA_CONFIRMATION\").agg(\n",
    "    F.count(\"*\").alias(\"COUNT\"),\n",
    "    F.avg(\"PFS_MONTHS\").alias(\"AVG_PFS\")\n",
    ").show()\n",
    "\n",
    "print(\"\\n=== Treatment Arm Performance ===\")\n",
    "clinical_df.group_by(\"TREATMENT_ARM\").agg(\n",
    "    F.count(\"*\").alias(\"COUNT\"),\n",
    "    F.avg(\"PFS_MONTHS\").alias(\"AVG_PFS\"),\n",
    "    F.avg(\"OS_MONTHS\").alias(\"AVG_OS\")\n",
    ").order_by(\"AVG_PFS\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-correlations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response rate by biomarker + ctDNA combination\n",
    "# This is the kind of insight that drives trial design\n",
    "\n",
    "response_analysis = clinical_df.with_column(\n",
    "    \"IS_RESPONDER\",\n",
    "    F.when(F.col(\"RESPONSE_CATEGORY\").isin([\"Complete_Response\", \"Partial_Response\"]), 1).otherwise(0)\n",
    ").group_by(\"BIOMARKER_STATUS\", \"CTDNA_CONFIRMATION\").agg(\n",
    "    F.count(\"*\").alias(\"PATIENTS\"),\n",
    "    F.sum(\"IS_RESPONDER\").alias(\"RESPONDERS\"),\n",
    "    (F.sum(\"IS_RESPONDER\") / F.count(\"*\") * 100).alias(\"RESPONSE_RATE_PCT\")\n",
    ").order_by(\"RESPONSE_RATE_PCT\", ascending=False)\n",
    "\n",
    "print(\"Response Rate by Biomarker + ctDNA Confirmation:\")\n",
    "response_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-store-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Snowflake Feature Store\n",
    "\n",
    "Create a **native Snowflake Feature Store** with:\n",
    "- Entities (Patient, Trial)\n",
    "- Feature Views with automatic refresh\n",
    "- Point-in-time correct feature retrieval\n",
    "\n",
    "**Why Feature Store matters for Life Sciences:**\n",
    "- Reproducibility for regulatory submissions\n",
    "- Consistent features across training and inference\n",
    "- Automatic lineage tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-store-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Feature Store\n",
    "# The Feature Store is just a schema - we'll create a dedicated one\n",
    "\n",
    "session.sql(\"CREATE SCHEMA IF NOT EXISTS LIFEARC_POC.ML_FEATURE_STORE\").collect()\n",
    "\n",
    "fs = FeatureStore(\n",
    "    session=session,\n",
    "    database=\"LIFEARC_POC\",\n",
    "    name=\"ML_FEATURE_STORE\",\n",
    "    default_warehouse=\"COMPUTE_WH\",\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    ")\n",
    "\n",
    "print(f\"✓ Feature Store initialized: {fs.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-entities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entities - the subjects of our features\n",
    "\n",
    "# Patient entity - each row represents a unique patient in a trial\n",
    "patient_entity = Entity(\n",
    "    name=\"PATIENT\",\n",
    "    join_keys=[\"PATIENT_ID\"],\n",
    "    desc=\"Individual patient enrolled in clinical trial\"\n",
    ")\n",
    "\n",
    "# Trial entity - for trial-level features\n",
    "trial_entity = Entity(\n",
    "    name=\"TRIAL\",\n",
    "    join_keys=[\"TRIAL_ID\"],\n",
    "    desc=\"Clinical trial identifier\"\n",
    ")\n",
    "\n",
    "# Register entities\n",
    "fs.register_entity(patient_entity)\n",
    "fs.register_entity(trial_entity)\n",
    "\n",
    "print(\"✓ Entities registered:\")\n",
    "for entity in fs.list_entities().to_pandas().itertuples():\n",
    "    print(f\"  - {entity.NAME}: {entity.JOIN_KEYS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Patient Feature View\n",
    "# These are features derived from patient-level clinical data\n",
    "\n",
    "patient_features_df = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        PATIENT_ID,\n",
    "        \n",
    "        -- Demographics (encoded)\n",
    "        PATIENT_AGE,\n",
    "        CASE WHEN PATIENT_AGE < 50 THEN 'YOUNG'\n",
    "             WHEN PATIENT_AGE < 65 THEN 'MIDDLE'\n",
    "             ELSE 'SENIOR' END AS AGE_GROUP,\n",
    "        \n",
    "        -- Biomarker features\n",
    "        BIOMARKER_STATUS,\n",
    "        CASE WHEN BIOMARKER_STATUS = 'POSITIVE' THEN 1 ELSE 0 END AS BIOMARKER_POSITIVE,\n",
    "        \n",
    "        -- ctDNA features  \n",
    "        CTDNA_CONFIRMATION,\n",
    "        CASE WHEN CTDNA_CONFIRMATION = 'YES' THEN 1 ELSE 0 END AS CTDNA_CONFIRMED,\n",
    "        \n",
    "        -- Treatment features\n",
    "        TREATMENT_ARM,\n",
    "        CASE TREATMENT_ARM \n",
    "            WHEN 'Combination' THEN 3\n",
    "            WHEN 'Experimental' THEN 2\n",
    "            WHEN 'Standard' THEN 1\n",
    "            ELSE 0 END AS TREATMENT_INTENSITY,\n",
    "        \n",
    "        -- Cohort features\n",
    "        COHORT,\n",
    "        \n",
    "        -- Timestamp for point-in-time correctness\n",
    "        CURRENT_TIMESTAMP() AS FEATURE_TIMESTAMP\n",
    "        \n",
    "    FROM LIFEARC_POC.DATA_SHARING.CLINICAL_TRIAL_RESULTS\n",
    "\"\"\")\n",
    "\n",
    "# Create feature view with managed refresh\n",
    "patient_fv = FeatureView(\n",
    "    name=\"PATIENT_CLINICAL_FEATURES\",\n",
    "    entities=[patient_entity],\n",
    "    feature_df=patient_features_df,\n",
    "    timestamp_col=\"FEATURE_TIMESTAMP\",\n",
    "    refresh_freq=\"1 day\",  # Auto-refresh daily\n",
    "    desc=\"Patient-level clinical features for response prediction\"\n",
    ")\n",
    "\n",
    "# Register the feature view\n",
    "patient_fv = fs.register_feature_view(\n",
    "    feature_view=patient_fv,\n",
    "    version=\"V1\",\n",
    "    block=True  # Wait for initial materialization\n",
    ")\n",
    "\n",
    "print(f\"✓ Feature View registered: {patient_fv.name}\")\n",
    "print(f\"  Features: {[f.name for f in patient_fv.feature_descs]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trial-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trial-level Feature View\n",
    "# Aggregate statistics per trial that can inform patient outcomes\n",
    "\n",
    "trial_features_df = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        TRIAL_ID,\n",
    "        \n",
    "        -- Trial performance metrics\n",
    "        COUNT(*) AS TRIAL_ENROLLMENT,\n",
    "        AVG(PFS_MONTHS) AS TRIAL_AVG_PFS,\n",
    "        STDDEV(PFS_MONTHS) AS TRIAL_STD_PFS,\n",
    "        AVG(OS_MONTHS) AS TRIAL_AVG_OS,\n",
    "        \n",
    "        -- Response rates by trial\n",
    "        SUM(CASE WHEN RESPONSE_CATEGORY IN ('Complete_Response', 'Partial_Response') \n",
    "            THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS TRIAL_RESPONSE_RATE,\n",
    "        \n",
    "        -- Biomarker prevalence in trial\n",
    "        SUM(CASE WHEN BIOMARKER_STATUS = 'POSITIVE' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) \n",
    "            AS TRIAL_BIOMARKER_POSITIVE_PCT,\n",
    "        \n",
    "        -- ctDNA usage in trial\n",
    "        SUM(CASE WHEN CTDNA_CONFIRMATION = 'YES' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) \n",
    "            AS TRIAL_CTDNA_USAGE_PCT,\n",
    "        \n",
    "        CURRENT_TIMESTAMP() AS FEATURE_TIMESTAMP\n",
    "        \n",
    "    FROM LIFEARC_POC.DATA_SHARING.CLINICAL_TRIAL_RESULTS\n",
    "    GROUP BY TRIAL_ID\n",
    "\"\"\")\n",
    "\n",
    "trial_fv = FeatureView(\n",
    "    name=\"TRIAL_AGGREGATE_FEATURES\",\n",
    "    entities=[trial_entity],\n",
    "    feature_df=trial_features_df,\n",
    "    timestamp_col=\"FEATURE_TIMESTAMP\",\n",
    "    refresh_freq=\"1 day\",\n",
    "    desc=\"Trial-level aggregate features\"\n",
    ")\n",
    "\n",
    "trial_fv = fs.register_feature_view(\n",
    "    feature_view=trial_fv,\n",
    "    version=\"V1\",\n",
    "    block=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Feature View registered: {trial_fv.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all feature views in our store\n",
    "print(\"=== Feature Store Contents ===\")\n",
    "fs.list_feature_views().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Generate Training Dataset\n",
    "\n",
    "Use the Feature Store to generate a **point-in-time correct** training dataset.\n",
    "\n",
    "This ensures:\n",
    "- No data leakage (features computed before label observation)\n",
    "- Reproducibility (same dataset can be regenerated)\n",
    "- Lineage tracking (automatic connection to models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spine-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spine table - the join keys and labels for training\n",
    "# This is the \"ground truth\" we're trying to predict\n",
    "\n",
    "spine_df = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        RESULT_ID,\n",
    "        PATIENT_ID,\n",
    "        TRIAL_ID,\n",
    "        \n",
    "        -- Target variable (what we're predicting)\n",
    "        RESPONSE_CATEGORY,\n",
    "        CASE RESPONSE_CATEGORY\n",
    "            WHEN 'Complete_Response' THEN 3\n",
    "            WHEN 'Partial_Response' THEN 2\n",
    "            WHEN 'Stable_Disease' THEN 1\n",
    "            WHEN 'Progressive_Disease' THEN 0\n",
    "            ELSE -1 END AS RESPONSE_LABEL,\n",
    "        \n",
    "        -- Binary target (responder vs non-responder)\n",
    "        CASE WHEN RESPONSE_CATEGORY IN ('Complete_Response', 'Partial_Response') \n",
    "            THEN 1 ELSE 0 END AS IS_RESPONDER,\n",
    "        \n",
    "        -- Timestamp for point-in-time join\n",
    "        CURRENT_TIMESTAMP() AS LABEL_TIMESTAMP\n",
    "        \n",
    "    FROM LIFEARC_POC.DATA_SHARING.CLINICAL_TRIAL_RESULTS\n",
    "    WHERE RESPONSE_CATEGORY IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Spine table rows: {spine_df.count():,}\")\n",
    "spine_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training dataset by joining features to spine\n",
    "# The Feature Store handles point-in-time correctness automatically\n",
    "\n",
    "training_dataset = fs.generate_dataset(\n",
    "    name=\"RESPONSE_PREDICTION_TRAINING\",\n",
    "    version=\"V1\",\n",
    "    spine_df=spine_df,\n",
    "    features=[\n",
    "        patient_fv,  # All patient features\n",
    "        trial_fv     # All trial features\n",
    "    ],\n",
    "    spine_timestamp_col=\"LABEL_TIMESTAMP\",\n",
    "    spine_label_cols=[\"RESPONSE_CATEGORY\", \"RESPONSE_LABEL\", \"IS_RESPONDER\"],\n",
    "    desc=\"Training dataset for clinical response prediction\"\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for training\n",
    "training_df = training_dataset.read.to_snowpark_dataframe()\n",
    "print(f\"\\n✓ Training dataset generated: {training_df.count():,} rows\")\n",
    "print(f\"  Columns: {training_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation/test sets\n",
    "# Using hash-based split for reproducibility\n",
    "\n",
    "train_df = training_df.filter(F.abs(F.hash(\"RESULT_ID\")) % 10 < 7)  # 70%\n",
    "val_df = training_df.filter((F.abs(F.hash(\"RESULT_ID\")) % 10 >= 7) & \n",
    "                            (F.abs(F.hash(\"RESULT_ID\")) % 10 < 9))  # 20%\n",
    "test_df = training_df.filter(F.abs(F.hash(\"RESULT_ID\")) % 10 >= 9)  # 10%\n",
    "\n",
    "print(f\"Training set: {train_df.count():,} rows\")\n",
    "print(f\"Validation set: {val_df.count():,} rows\")\n",
    "print(f\"Test set: {test_df.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Model Training with Snowpark ML\n",
    "\n",
    "Train a classification model using **Snowpark ML** - no data leaves Snowflake.\n",
    "\n",
    "Benefits:\n",
    "- Data stays under governance\n",
    "- Distributed training on Snowflake compute\n",
    "- Native integration with Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for training\n",
    "NUMERIC_FEATURES = [\n",
    "    \"PATIENT_AGE\",\n",
    "    \"BIOMARKER_POSITIVE\",\n",
    "    \"CTDNA_CONFIRMED\",\n",
    "    \"TREATMENT_INTENSITY\",\n",
    "    \"TRIAL_ENROLLMENT\",\n",
    "    \"TRIAL_AVG_PFS\",\n",
    "    \"TRIAL_RESPONSE_RATE\",\n",
    "    \"TRIAL_BIOMARKER_POSITIVE_PCT\",\n",
    "    \"TRIAL_CTDNA_USAGE_PCT\"\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"AGE_GROUP\",\n",
    "    \"BIOMARKER_STATUS\",\n",
    "    \"TREATMENT_ARM\",\n",
    "    \"COHORT\"\n",
    "]\n",
    "\n",
    "TARGET = \"IS_RESPONDER\"  # Binary classification: responder vs non-responder\n",
    "\n",
    "print(f\"Numeric features: {len(NUMERIC_FEATURES)}\")\n",
    "print(f\"Categorical features: {len(CATEGORICAL_FEATURES)}\")\n",
    "print(f\"Target: {TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Snowpark ML Pipeline\n",
    "# This runs entirely within Snowflake\n",
    "\n",
    "# Preprocessing: Scale numeric, encode categorical\n",
    "scaler = StandardScaler(\n",
    "    input_cols=NUMERIC_FEATURES,\n",
    "    output_cols=[f\"{c}_SCALED\" for c in NUMERIC_FEATURES]\n",
    ")\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    input_cols=CATEGORICAL_FEATURES,\n",
    "    output_cols=[f\"{c}_ENCODED\" for c in CATEGORICAL_FEATURES],\n",
    "    drop_input_cols=True\n",
    ")\n",
    "\n",
    "# Model: XGBoost Classifier\n",
    "model = XGBClassifier(\n",
    "    input_cols=[f\"{c}_SCALED\" for c in NUMERIC_FEATURES] + \n",
    "               [f\"{c}_ENCODED\" for c in CATEGORICAL_FEATURES],\n",
    "    label_cols=[TARGET],\n",
    "    output_cols=[\"PREDICTION\"],\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Assemble pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"scaler\", scaler),\n",
    "    (\"encoder\", encoder),\n",
    "    (\"classifier\", model)\n",
    "])\n",
    "\n",
    "print(\"✓ ML Pipeline defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# All computation happens in Snowflake warehouse\n",
    "\n",
    "print(\"Training model... (this runs on Snowflake compute)\")\n",
    "pipeline.fit(train_df)\n",
    "print(\"✓ Model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "val_predictions = pipeline.predict(val_df)\n",
    "\n",
    "# Calculate metrics\n",
    "val_pdf = val_predictions.select(TARGET, \"PREDICTION\").to_pandas()\n",
    "\n",
    "accuracy = accuracy_score(val_pdf[TARGET], val_pdf[\"PREDICTION\"])\n",
    "precision = precision_score(val_pdf[TARGET], val_pdf[\"PREDICTION\"])\n",
    "recall = recall_score(val_pdf[TARGET], val_pdf[\"PREDICTION\"])\n",
    "f1 = f1_score(val_pdf[TARGET], val_pdf[\"PREDICTION\"])\n",
    "\n",
    "print(\"=== Validation Metrics ===\")\n",
    "print(f\"Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1 Score:  {f1:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(val_pdf[TARGET], val_pdf[\"PREDICTION\"])\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN={cm[0][0]}, FP={cm[0][1]}\")\n",
    "print(f\"  FN={cm[1][0]}, TP={cm[1][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registry-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Snowflake Model Registry\n",
    "\n",
    "Register the trained model in the **native Snowflake Model Registry**.\n",
    "\n",
    "Capabilities:\n",
    "- Version control with semantic versioning\n",
    "- Aliases for lifecycle stages (dev, staging, production)\n",
    "- Metrics tracking\n",
    "- Role-based access control\n",
    "- Automatic lineage to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-registry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model Registry\n",
    "registry = Registry(\n",
    "    session=session,\n",
    "    database_name=\"LIFEARC_POC\",\n",
    "    schema_name=\"ML_DEMO\"\n",
    ")\n",
    "\n",
    "print(\"✓ Model Registry initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "log-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log model to registry with metrics\n",
    "model_version = registry.log_model(\n",
    "    model=pipeline,\n",
    "    model_name=\"CLINICAL_RESPONSE_PREDICTOR\",\n",
    "    version_name=\"V1\",\n",
    "    metrics={\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"training_rows\": train_df.count(),\n",
    "        \"validation_rows\": val_df.count()\n",
    "    },\n",
    "    comment=\"XGBoost classifier for predicting clinical trial response\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Model logged: {model_version.model_name} version {model_version.version_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "set-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set alias for lifecycle management\n",
    "# This allows production code to always call 'production' version\n",
    "\n",
    "model_version.set_alias(\"development\")\n",
    "print(\"✓ Alias 'development' set\")\n",
    "\n",
    "# After validation, promote to production:\n",
    "# model_version.set_alias(\"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all models in registry\n",
    "print(\"=== Model Registry Contents ===\")\n",
    "registry.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed model information\n",
    "model = registry.get_model(\"CLINICAL_RESPONSE_PREDICTOR\")\n",
    "print(f\"Model: {model.name}\")\n",
    "print(f\"\\nVersions:\")\n",
    "for version in model.versions():\n",
    "    print(f\"  - {version.version_name}\")\n",
    "    print(f\"    Metrics: {version.get_metric('accuracy'):.3f} accuracy\")\n",
    "    print(f\"    Aliases: {version.list_aliases()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Model Inference\n",
    "\n",
    "Deploy the model for batch and real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference on test set\n",
    "# Retrieve model from registry and run predictions\n",
    "\n",
    "model_ref = registry.get_model(\"CLINICAL_RESPONSE_PREDICTOR\").version(\"V1\")\n",
    "\n",
    "# Run inference\n",
    "test_predictions = model_ref.run(test_df, function_name=\"predict\")\n",
    "\n",
    "print(\"=== Test Set Predictions ===\")\n",
    "test_predictions.select(\n",
    "    \"RESULT_ID\", \n",
    "    \"PATIENT_ID\", \n",
    "    TARGET, \n",
    "    \"PREDICTION\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test metrics\n",
    "test_pdf = test_predictions.select(TARGET, \"PREDICTION\").to_pandas()\n",
    "\n",
    "test_accuracy = accuracy_score(test_pdf[TARGET], test_pdf[\"PREDICTION\"])\n",
    "test_precision = precision_score(test_pdf[TARGET], test_pdf[\"PREDICTION\"])\n",
    "test_recall = recall_score(test_pdf[TARGET], test_pdf[\"PREDICTION\"])\n",
    "test_f1 = f1_score(test_pdf[TARGET], test_pdf[\"PREDICTION\"])\n",
    "\n",
    "print(\"=== Test Set Metrics (Holdout) ===\")\n",
    "print(f\"Accuracy:  {test_accuracy:.3f}\")\n",
    "print(f\"Precision: {test_precision:.3f}\")\n",
    "print(f\"Recall:    {test_recall:.3f}\")\n",
    "print(f\"F1 Score:  {test_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sql-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL-based inference (for production use)\n",
    "# This is how downstream systems would call the model\n",
    "\n",
    "sql_inference_example = \"\"\"\n",
    "-- Call model directly from SQL\n",
    "WITH patient_features AS (\n",
    "    SELECT * FROM LIFEARC_POC.ML_FEATURE_STORE.PATIENT_CLINICAL_FEATURES$V1\n",
    "),\n",
    "trial_features AS (\n",
    "    SELECT * FROM LIFEARC_POC.ML_FEATURE_STORE.TRIAL_AGGREGATE_FEATURES$V1\n",
    ")\n",
    "SELECT \n",
    "    p.PATIENT_ID,\n",
    "    LIFEARC_POC.ML_DEMO.CLINICAL_RESPONSE_PREDICTOR!PREDICT(\n",
    "        OBJECT_CONSTRUCT(*)\n",
    "    ) AS PREDICTION\n",
    "FROM patient_features p\n",
    "JOIN trial_features t ON p.TRIAL_ID = t.TRIAL_ID\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"SQL Inference Pattern:\")\n",
    "print(sql_inference_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitoring-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Model Monitoring & Observability\n",
    "\n",
    "Set up monitoring for:\n",
    "- Prediction drift\n",
    "- Feature distribution changes\n",
    "- Performance degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-monitoring-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create monitoring table for tracking predictions over time\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS LIFEARC_POC.ML_DEMO.MODEL_MONITORING (\n",
    "        MONITORING_ID VARCHAR DEFAULT UUID_STRING(),\n",
    "        MODEL_NAME VARCHAR,\n",
    "        MODEL_VERSION VARCHAR,\n",
    "        MONITORING_DATE DATE,\n",
    "        TOTAL_PREDICTIONS INT,\n",
    "        POSITIVE_PREDICTIONS INT,\n",
    "        NEGATIVE_PREDICTIONS INT,\n",
    "        POSITIVE_RATE FLOAT,\n",
    "        AVG_CONFIDENCE FLOAT,\n",
    "        FEATURE_STATS VARIANT,\n",
    "        CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n",
    "    )\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"✓ Monitoring table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "log-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log today's prediction statistics\n",
    "prediction_stats = test_predictions.agg(\n",
    "    F.count(\"*\").alias(\"TOTAL\"),\n",
    "    F.sum(F.when(F.col(\"PREDICTION\") == 1, 1).otherwise(0)).alias(\"POSITIVE\"),\n",
    "    F.sum(F.when(F.col(\"PREDICTION\") == 0, 1).otherwise(0)).alias(\"NEGATIVE\")\n",
    ").collect()[0]\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "    INSERT INTO LIFEARC_POC.ML_DEMO.MODEL_MONITORING \n",
    "    (MODEL_NAME, MODEL_VERSION, MONITORING_DATE, TOTAL_PREDICTIONS, \n",
    "     POSITIVE_PREDICTIONS, NEGATIVE_PREDICTIONS, POSITIVE_RATE)\n",
    "    VALUES (\n",
    "        'CLINICAL_RESPONSE_PREDICTOR',\n",
    "        'V1',\n",
    "        CURRENT_DATE(),\n",
    "        {prediction_stats['TOTAL']},\n",
    "        {prediction_stats['POSITIVE']},\n",
    "        {prediction_stats['NEGATIVE']},\n",
    "        {prediction_stats['POSITIVE'] / prediction_stats['TOTAL']:.4f}\n",
    "    )\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"✓ Prediction statistics logged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drift-detection-sql",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift detection query\n",
    "# Compare current predictions to historical baseline\n",
    "\n",
    "drift_query = \"\"\"\n",
    "-- Detect prediction drift\n",
    "WITH baseline AS (\n",
    "    SELECT AVG(POSITIVE_RATE) AS baseline_rate\n",
    "    FROM LIFEARC_POC.ML_DEMO.MODEL_MONITORING\n",
    "    WHERE MODEL_NAME = 'CLINICAL_RESPONSE_PREDICTOR'\n",
    "      AND MONITORING_DATE < CURRENT_DATE() - 7\n",
    "),\n",
    "current AS (\n",
    "    SELECT AVG(POSITIVE_RATE) AS current_rate\n",
    "    FROM LIFEARC_POC.ML_DEMO.MODEL_MONITORING\n",
    "    WHERE MODEL_NAME = 'CLINICAL_RESPONSE_PREDICTOR'\n",
    "      AND MONITORING_DATE >= CURRENT_DATE() - 7\n",
    ")\n",
    "SELECT \n",
    "    b.baseline_rate,\n",
    "    c.current_rate,\n",
    "    ABS(c.current_rate - b.baseline_rate) AS drift,\n",
    "    CASE \n",
    "        WHEN ABS(c.current_rate - b.baseline_rate) > 0.1 THEN 'ALERT: Significant drift detected'\n",
    "        WHEN ABS(c.current_rate - b.baseline_rate) > 0.05 THEN 'WARNING: Moderate drift'\n",
    "        ELSE 'OK: Within normal range'\n",
    "    END AS status\n",
    "FROM baseline b, current c;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Drift Detection Query:\")\n",
    "print(drift_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scheduled task for continuous monitoring\n",
    "monitoring_task_sql = \"\"\"\n",
    "-- Create scheduled monitoring task\n",
    "CREATE OR REPLACE TASK LIFEARC_POC.ML_DEMO.MONITOR_MODEL_PREDICTIONS\n",
    "    WAREHOUSE = COMPUTE_WH\n",
    "    SCHEDULE = 'USING CRON 0 8 * * * UTC'  -- Daily at 8 AM UTC\n",
    "AS\n",
    "CALL LIFEARC_POC.ML_DEMO.LOG_PREDICTION_STATS();\n",
    "\n",
    "-- To enable:\n",
    "-- ALTER TASK LIFEARC_POC.ML_DEMO.MONITOR_MODEL_PREDICTIONS RESUME;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Scheduled Monitoring Task:\")\n",
    "print(monitoring_task_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lineage-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: ML Lineage\n",
    "\n",
    "Snowflake ML automatically tracks lineage from:\n",
    "- Source data → Features → Training Dataset → Model\n",
    "\n",
    "This is critical for:\n",
    "- Regulatory compliance (21 CFR Part 11)\n",
    "- Reproducibility\n",
    "- Debugging production issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-lineage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query lineage information\n",
    "lineage_query = \"\"\"\n",
    "-- View model lineage\n",
    "SELECT \n",
    "    OBJECT_NAME,\n",
    "    OBJECT_DATABASE,\n",
    "    OBJECT_SCHEMA,\n",
    "    OBJECT_TYPE,\n",
    "    UPSTREAM_OBJECT_NAME,\n",
    "    UPSTREAM_OBJECT_TYPE\n",
    "FROM TABLE(INFORMATION_SCHEMA.OBJECT_DEPENDENCIES(\n",
    "    OBJECT_NAME => 'CLINICAL_RESPONSE_PREDICTOR',\n",
    "    OBJECT_TYPE => 'MODEL'\n",
    "));\n",
    "\"\"\"\n",
    "\n",
    "print(\"Lineage Query:\")\n",
    "print(lineage_query)\n",
    "\n",
    "# In Snowsight, you can also visualize lineage graphically\n",
    "print(\"\\nTip: View lineage graph in Snowsight > Data > ML_DEMO > Models > CLINICAL_RESPONSE_PREDICTOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Complete ML Lifecycle in Snowflake\n",
    "\n",
    "This notebook demonstrated end-to-end ML using **native Snowflake capabilities**:\n",
    "\n",
    "| Stage | What We Built | Snowflake Feature |\n",
    "|-------|---------------|-------------------|\n",
    "| 1. Discovery | Explored clinical trial data | Snowpark DataFrames |\n",
    "| 2. Features | Patient & Trial feature views | **Snowflake Feature Store** |\n",
    "| 3. Training | XGBoost classification pipeline | Snowpark ML |\n",
    "| 4. Registry | Versioned model with metrics | **Snowflake Model Registry** |\n",
    "| 5. Inference | Batch predictions via SQL | Model Registry |\n",
    "| 6. Monitoring | Drift detection | ML Observability |\n",
    "| 7. Lineage | Source-to-model traceability | **ML Lineage** |\n",
    "\n",
    "### Objects Created\n",
    "\n",
    "```\n",
    "LIFEARC_POC.ML_FEATURE_STORE/\n",
    "├── PATIENT entity\n",
    "├── TRIAL entity\n",
    "├── PATIENT_CLINICAL_FEATURES (Feature View)\n",
    "└── TRIAL_AGGREGATE_FEATURES (Feature View)\n",
    "\n",
    "LIFEARC_POC.ML_DEMO/\n",
    "├── CLINICAL_RESPONSE_PREDICTOR (Model)\n",
    "│   └── V1 (Version, alias: development)\n",
    "└── MODEL_MONITORING (Table)\n",
    "```\n",
    "\n",
    "### Why This Matters for Life Sciences\n",
    "\n",
    "1. **Regulatory Compliance**: Full lineage from source to prediction\n",
    "2. **Data Governance**: PHI never leaves Snowflake\n",
    "3. **Reproducibility**: Feature Store ensures consistent features\n",
    "4. **Auditability**: Model Registry tracks all versions\n",
    "5. **Operationalization**: SQL-based inference for production systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close session if running locally\n",
    "# session.close()\n",
    "print(\"\\n✓ Notebook complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
