Preface:Thanks for following up. As requested, please find a refined list of the use cases attached to this email that we would like to cover in the upcoming development workshop. Please let me know if the team has any follow-up questions ahead of the session.The intent is to structure the workshop as a working discussion, where we can walk through each use case and, at a minimum, align on what a viable Snowflake-based solution could look like. This will allow our team to either probe further with questions or gain clarity on how best to leverage Snowflake in each scenario.For the first three use cases, our primary objective is to understand the recommended solution architecture with Snowflake as part of the overall design. For items 4, 5, and 6, it would be extremely helpful if these could be supported by a demonstration or hands-on walkthrough showing how such scenarios are typically handled in Snowflake. Please let us know if you need any further clarification ahead of the session. We’re looking forward to a deep-dive discussion on these topics.1. Enterprise Data Archiving using Snowflake and Azure Blob StorageWe currently archive enterprise-wide historical data on a platform called Ctera. We are exploring replacing this with a Snowflake-centric architecture, where:Snowflake acts as the system of record for metadata, indexing, and controlled accessAzure Blob Storage is used for warm and cold storage of large archival datasetsCurrent data volume: ~250TB and growingKey areas we would like to explore:Reference architectures for Snowflake-based data archivingBest practices for managing metadata, discoverability, and access control for archived dataCost, performance, and governance considerations when combining Snowflake with external object storagePatterns for retrieving and rehydrating archived data when needed2. Designing MLOps Workflows with SnowflakeFrom a data science perspective, we would like guidance on how best to design MLOps workflows that align with Snowflake’s processing model while maintaining productivity for ML teams.Today, our ML projects are typically structured as GitHub repositories organised as Python packages, containing notebooks and reusable modules to:Read data from source systems or VMsPerform data quality checks and validationApply feature engineering and transformationsWrite curated datasets and model outputs back to storageThe implementation varies by dataset and product. We understand Snowflake is optimised for set-based, SQL-driven processing, which works well for many transformations. However, some of our ML use cases require:Hierarchical or nested data structuresGroup-level, sequence-based, or windowed transformationsLogic that is more naturally expressed procedurally rather than purely in SQLWe would like guidance on:Recommended patterns when processing is split between Snowflake and external Python environmentsWhich stages are best handled natively in Snowflake versus externally (e.g. feature engineering, model training, inference)Approaches for handling hierarchical or semi-structured data efficiently without row-by-row processingHow Snowflake features (e.g. Snowpark, tasks, streams) fit into a scalable MLOps designThe goal is to establish a clear, scalable approach that supports ML workflows while remaining aligned with Snowflake best practices.3. Integration with Azure ML StudioWe currently use Azure ML Studio for ML development and deployment, including:Compute on VMsAzure Blob StorageModel registry and data registryModel deployment and lifecycle managementGiven this existing setup, we would like to understand:How Snowflake can be integrated into this architecture following best practicesWhether Snowflake can act purely as a centralised data and feature store while compute remains in Azure MLRecommended integration patterns for data access, security, and lineage between Snowflake and Azure MLOur intent is to centralise data and its associated artefacts while minimising disruption to existing ML workflows.4. Handling Unstructured and Semi-Structured Data in SnowflakeWe work with a range of unstructured and semi-structured data types, including:Text data used for LLM pipelinesFASTA and FASTQ files for sequencing dataSDF files for molecular structure and 3D renderingNIfTI and DICOM files for imaging dataGiven these requirements, we would like to understand:How Snowflake supports storage, metadata management, and access for such data typesRecommended patterns for managing and processing unstructured and semi-structured dataAny practical examples or customer use cases involving similar data typesAny hands-on demonstrations or reference implementations would be particularly helpful.5. Data Contracts, Data Sharing, and Policy ControlsWe would like to explore how Snowflake supports data contracts, governance, and policy enforcement for data shared across external partners, including scenarios where partners:Use their own Snowflake accounts and share data nativelyProvide data via physical media (e.g. hard drives, USBs)Are given access to a cloud location or Snowflake-managed environment to upload dataKey areas of interest include access control, auditing, data ownership, and lifecycle management.6. Programmatic Access and AuthenticationWe are aware of Snowflake’s role-based access control (RBAC). In addition, we would like to understand:Options for non-user-based, programmatic accessSupport for mechanisms such as key-pair authentication, OAuth, or token-based accessBest practices for securing service-to-service access in automated pipelines